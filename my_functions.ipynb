{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from  sklearn.metrics import accuracy_score \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,KFold, learning_curve,StratifiedKFold\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=4, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "def detect_outliers(df, n, features):\n",
    "    \"\"\"\n",
    "    Takes a dataframe df of features and returns a list of the indices\n",
    "    corresponding to the observations containing more than n outliers according\n",
    "    to the Tukey method.\n",
    "    \"\"\"\n",
    "    outlier_indices = []\n",
    "\n",
    "    # iterate over features(columns)\n",
    "    for col in features:\n",
    "        # 1st quartile (25%)\n",
    "        Q1 = np.percentile(df[col], 25)\n",
    "        # 3rd quartile (75%)\n",
    "        Q3 = np.percentile(df[col], 75)\n",
    "        # Interquartile range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # outlier step\n",
    "        outlier_step = 1.5 * IQR\n",
    "\n",
    "        # Determine a list of indices of outliers for feature col\n",
    "        outlier_list_col = df[(df[col] < Q1 - outlier_step) |\n",
    "                              (df[col] > Q3 + outlier_step)].index\n",
    "\n",
    "        # append the found outlier indices for col to the list of outlier indices\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "    # select observations containing more than 2 outliers\n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(k for k, v in outlier_indices.items() if v > n)\n",
    "\n",
    "    return multiple_outliers\n",
    "\n",
    "\n",
    "def cross_validation_score_statement(estimator,X,y,scoring,n_splits=5,statement=None,random_state=0):\n",
    "    \"\"\"\n",
    "    Evaluate a score by cross-validation. \n",
    "    The fit method will be performed on the entire train subset at each iteration,\n",
    "    the predict method and scoring will be performed only for objects from test subset where statement is True\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit' and 'predict'\n",
    "        The object to use to fit the data.\n",
    "    X : pandas.DataFrame\n",
    "        The data to fit.\n",
    "    y : pandas.Series\n",
    "        The target variable to try to predict.\n",
    "    scoring : callable \n",
    "        The scoring function of signature scoring(y_true,y_pred).\n",
    "    statement : boolean numpy.array of shape equal to y.shape\n",
    "        The mask showing the objects we want to evaluate estimator on.\n",
    "    n_splits : int\n",
    "        Number of folds for cross-validation\n",
    "    random_state : int\n",
    "        Random_state for KFold and StratifiedKFold    \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    scores : array of float, shape=(n_splits,)\n",
    "    \n",
    "    \"\"\"\n",
    "    if statement is None:\n",
    "        cv = KFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "        cv_iter = list(cv.split(X, y))\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=random_state)\n",
    "        cv_iter = list(cv.split(X, statement))\n",
    "    scores = []\n",
    "    \n",
    "    for train, test in cv_iter:\n",
    "        estimator.fit(X.iloc[train,:].values,y.iloc[train].values)\n",
    "        if statement is not None:\n",
    "            y_statement = y.iloc[test].loc[statement[test]]\n",
    "            pred_statement = estimator.predict(X.iloc[test,:].loc[statement[test]].values)\n",
    "        else:\n",
    "            y_statement = y.iloc[test]\n",
    "            pred_statement = estimator.predict(X.iloc[test,:].values)\n",
    "        scores.append(scoring(y_statement,pred_statement))\n",
    "    return np.array(scores)\n",
    "\n",
    "def fit_models(X_train,Y_train):\n",
    "    kfold = KFold(n_splits=10, shuffle=True,random_state=241)\n",
    "    random_state = 241\n",
    "    classifiers = []\n",
    "    classifiers.append(SVC(random_state=random_state))\n",
    "    classifiers.append(DecisionTreeClassifier(random_state=random_state))\n",
    "    classifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\n",
    "    classifiers.append(RandomForestClassifier(random_state=241)) \n",
    "    classifiers.append(ExtraTreesClassifier(random_state=random_state))\n",
    "    classifiers.append(GradientBoostingClassifier(random_state=random_state,criterion='friedman_mse', init=None,\n",
    "              learning_rate=0.1, loss='deviance', max_depth=8,\n",
    "              max_features=0.3, max_leaf_nodes=None,\n",
    "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "              min_samples_leaf=100, min_samples_split=2,\n",
    "              min_weight_fraction_leaf=0.0, n_estimators=300,\n",
    "              n_iter_no_change=None, presort='auto',\n",
    "              subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "              verbose=0, warm_start=False))\n",
    "    classifiers.append(MLPClassifier(random_state=random_state))\n",
    "    classifiers.append(KNeighborsClassifier())\n",
    "    classifiers.append(LogisticRegression(random_state = random_state,C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
    "          n_jobs=None, penalty='l1', solver='warn',\n",
    "          tol=0.0001, verbose=0, warm_start=False))\n",
    "    classifiers.append(LinearDiscriminantAnalysis())\n",
    "    classifiers.append(XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.8, gamma=1, learning_rate=0.02, max_delta_step=0,\n",
    "       max_depth=4, min_child_weight=1, missing=None, n_estimators=600,\n",
    "       n_jobs=1, nthread=1, objective='binary:logistic',\n",
    "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
    "       silent=True, subsample=0.6,random_state=random_state))\n",
    "    \n",
    "    cv_results = []\n",
    "    for classifier in classifiers :\n",
    "        cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"f1_macro\", cv = kfold, n_jobs=4))\n",
    "    \n",
    "    cv_means = []\n",
    "    cv_std = []\n",
    "    for cv_result in cv_results:\n",
    "        cv_means.append(cv_result.mean())\n",
    "        cv_std.append(cv_result.std())\n",
    "    \n",
    "    cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n",
    "    \"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"\n",
    "                                                                                          ,\"XGB\"]})\n",
    "    return cv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
